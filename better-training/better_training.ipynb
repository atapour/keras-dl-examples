{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Lecture10_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Keras code to demonstrate better training of neural networks using CIFAR-10\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@newcastle.ac.uk\n",
        "\n",
        "This notebook will provide an example to show how data augmentation and transfer learning can improve the performance of neural networks and prevent overfitting using the CIFAR-10 dataset.\n",
        "\n",
        "This is a code demonstration for CSC8637: Deep Learning module, Lecture 10: Better Training of Neural Networks.\n",
        "\n",
        "Copyright (c) 2021 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ],
      "metadata": {
        "id": "HiyK9v7m3uOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As with our other code demonstrations in our lecture series, we will be using Keras to train a model on the **CIFAR-10** dataset.\n",
        "\n",
        "This dataset contains 50,000 training images and 10,000 test images. In the following snippet, we first load and explore the dataset by visualising the first few images within the dataset:"
      ],
      "metadata": {
        "id": "bvYyOmMF776K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Required imports:\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# The dataset contains 50,000 training images and 10,000 test images.\n",
        "# Loading the dataset:\n",
        "print('CIFAR-10 Dataset!')\n",
        "(train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
        "\n",
        "# CIFAR-10 contains these classes:\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'lorry']\n",
        "\n",
        "# -----------------------------\n",
        "# This function will display the first 16 images of the dataset with their labels:\n",
        "def visualize_data(train_X, train_Y, class_names):\n",
        "\n",
        "  for i in range(16):\n",
        "    # create subplot:\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # plot image with the class name on the x-axis:\n",
        "    plt.imshow(train_X[i])\n",
        "    plt.xlabel(class_names[train_Y[i].item()])\n",
        "\n",
        "  # adjust the subplots and show the first 16 images:\n",
        "  plt.subplots_adjust(left=0.125,\n",
        "                      bottom=0.1, \n",
        "                      right=0.9, \n",
        "                      top=0.9, \n",
        "                      wspace=0.2, \n",
        "                      hspace=0.35)\n",
        "  plt.show()\n",
        "# -----------------------------\n",
        "\n",
        "# Displaying the first sixteen images within the dataset:\n",
        "visualize_data(train_X, train_Y, class_names)\n",
        "\n",
        "# Printing information about the loaded dataset:\n",
        "print(f'There are {train_X.shape[0]} images of size {train_X.shape[1:]} in the Training set of the CIFAR-10 Dataset.')\n",
        "print(f'There are {test_X.shape[0]} images of size {test_X.shape[1:]} in the Test set of the CIFAR-10 Dataset.')"
      ],
      "outputs": [],
      "metadata": {
        "id": "L9DHVccy2RdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we want to demonstrate how we can prevent overfitting. As overfitting often happens when we are dealing with a small dataset, we are only going to use 5,000 of the training image in CIFAR-10:"
      ],
      "metadata": {
        "id": "aldtIggPZ0iO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_X, train_Y = train_X[:5000], train_Y[:5000]\n",
        "\n",
        "print(f'We have selected the first {train_X.shape[0]} images of size {train_X.shape[1:]} for our training dataset.')"
      ],
      "outputs": [],
      "metadata": {
        "id": "_Pxs7DsHaNCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have had a look at the dataset we will be working with, we can begin preparing for the training process. First, let's import all that we need:"
      ],
      "metadata": {
        "id": "jtyLC61r2tZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print('Keras version:', keras.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "outputs": [],
      "metadata": {
        "id": "f2qiRCTTLdpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are going to be running experiments here, we would like our results to be reproducible. So we need to make our setup as deterministic as possible. One thing that will help with this is setting seeds for any and all randomness in our code.\n",
        "\n",
        "Note that especially since we are running our code on **GPU** cloud services, there are variables that we can't always control (easily) so reaching absolute determinism won't really be possible here. But setting seeds is always good practice.\n",
        "\n",
        "Additionally, by setting seeds, we can control the stochasticity of our algorithm (for example, the effects of the random initialisation of our network weights). One can run a given experiment a number of times with different random seeds and then average the performance metrics across all the runs.\n",
        "\n",
        "Note that different packages and libraries might have different random number generation processes. Here, we will pick a seed value of 0 for Numpy, TensorFlow, the Python Random module:"
      ],
      "metadata": {
        "id": "vxYod4NV3XwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random as rn\n",
        "\n",
        "seed_value = 0\n",
        "\n",
        "np.random.seed(seed_value)\n",
        "rn.seed(seed_value)\n",
        "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "tf.compat.v1.set_random_seed(seed_value)\n",
        "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
        "tf.compat.v1.keras.backend.set_session(sess)\n",
        "\n",
        "print('Done!')"
      ],
      "outputs": [],
      "metadata": {
        "id": "ScrORReR3tdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first choose the architecture we are going to use. We are going to use VGG16. In our first experiment, the model is **not pre-trained** and will use **no data augmentation**:"
      ],
      "metadata": {
        "id": "ntLFv-w-bW9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# This function defines our classification network:\n",
        "def create_model():\n",
        "\n",
        "  base_model = VGG16(input_shape=(32,32,3), weights=None, include_top=False)\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  output = keras.layers.Dense(10, activation='softmax')(x)\n",
        "  model = keras.models.Model(inputs=[base_model.input], outputs=[output])\n",
        "\n",
        "  # The optimiser is stochastic gradient descent with a learning rate of 0.01 and a momentum of 0.9:\n",
        "  optim = SGD(lr=0.01, momentum=0.9)\n",
        "\n",
        "  # The model optimises cross entropy as its loss function and will monitor classification accuracy:\n",
        "  model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Printing model summary:\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "print('Done!')"
      ],
      "outputs": [],
      "metadata": {
        "id": "gNJZNBPvbdfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define two functions that will load and process the data in preparation for training. Remember that we are only going to use 5,000 images for training:"
      ],
      "metadata": {
        "id": "khsYp2iGNWHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Function used to load the dataset:\n",
        "def load_data():\n",
        "\n",
        "  # Loading the built-in CIFAR-10 dataset from Keras:\n",
        "  (train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
        "\n",
        "  # We are going to use only 5,000 images for training:\n",
        "  train_X, train_Y = train_X[:5000], train_Y[:5000]\n",
        "\n",
        "  print('The dataset has been loaded:')\n",
        "  print(f'Train: X={train_X.shape}, Y={train_Y.shape}')\n",
        "  print(f'Test: X={test_X.shape}, Y={test_Y.shape}')\n",
        "\n",
        "  # Converting class labels to one hot vectors:\n",
        "  train_Y = to_categorical(train_Y)\n",
        "  test_Y = to_categorical(test_Y)\n",
        "\n",
        "  return train_X, train_Y, test_X, test_Y\n",
        " \n",
        "#  ---------------------------------------\n",
        "\n",
        "# Function used to pre-process the data:\n",
        "def pre_process_data(train_data, test_data):\n",
        "\n",
        "  # Casting pixel values to floats:\n",
        "  train_data = train_data.astype('float32')\n",
        "  test_data = test_data.astype('float32')\n",
        "\n",
        "  # Normalising pixel values to range [0-1]\n",
        "  train_data = train_data / 255.0\n",
        "  test_data = test_data / 255.0\n",
        "\n",
        "  print(f'Train data is in range {train_data.min()} to {train_data.max()}.')\n",
        "  print(f'Test data is in range {test_data.min()} to {test_data.max()}.')\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "print('Done!')"
      ],
      "outputs": [],
      "metadata": {
        "id": "UwcJww6zNwOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also define a function responsible for plotting the curves:"
      ],
      "metadata": {
        "id": "IDBCjcmWbxzv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Function used to plot the curves for loss and accuracy:\n",
        "def plot_curves(history):\n",
        "\n",
        "  # Plotting the loss curve:\n",
        "  plt.subplot(211)\n",
        "  plt.title('Cross Entropy')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  # Plotting the training loss (blue):\n",
        "  plt.plot(history.history['loss'], color='blue', label='train')\n",
        "  # Plotting the test loss (red):\n",
        "  plt.plot(history.history['val_loss'], color='red', label='test')\n",
        "  # Legend for the plot:\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # Plotting the accuracy curve:\n",
        "  plt.subplot(212)\n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  # Plotting the training accuracy (blue):\n",
        "  plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "  # Plotting the test accuracy (red):\n",
        "  plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
        "  # Legend for the plot:\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  plt.subplots_adjust(top=3)\n",
        "  plt.show()\n",
        "\n",
        "print('Done!')"
      ],
      "outputs": [],
      "metadata": {
        "id": "8rnUNcFYb5iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that all preparations are made, it is time to start training the model."
      ],
      "metadata": {
        "id": "q7_ukGSQc4mz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's load the dataset. Note that the size of our training data is very small (only 5,000 images and half the size of our test data), a situation ripe for overfitting:"
      ],
      "metadata": {
        "id": "jn3VXARvdE7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_X, train_Y, test_X, test_Y = load_data()"
      ],
      "outputs": [],
      "metadata": {
        "id": "jxLL8PGddJfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we pre-process the images:"
      ],
      "metadata": {
        "id": "o5HA9zq5dhcq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "train_X, test_X = pre_process_data(train_X, test_X)"
      ],
      "outputs": [],
      "metadata": {
        "id": "vl05vwsEduA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the model:"
      ],
      "metadata": {
        "id": "xztA7dmCfkLB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "model = create_model()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Q2bH8teBfmq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to train the model for 50 epochs (more than what we need with respect to the size of our training set) with a batch size of 64:"
      ],
      "metadata": {
        "id": "3lIJ2EJueYZC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "history = model.fit(train_X, train_Y, epochs=50, batch_size=64, validation_data=(test_X, test_Y))"
      ],
      "outputs": [],
      "metadata": {
        "id": "cNosfjAueY4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the training is done, we can evaluate the model on the test set and see what final accuracy levels we get:"
      ],
      "metadata": {
        "id": "44EK0qiahETH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "_, acc = model.evaluate(test_X, test_Y, verbose=1)\n",
        "print('Accuracy: %.3f' % (acc * 100.0))"
      ],
      "outputs": [],
      "metadata": {
        "id": "dIDsBvm2hLbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the loss and accuracy curves will help us better analyse the training process.\n",
        "\n",
        "The **blue** curves indicate performance over the **training data** and the *red* curves represent model performance over the *test data*:"
      ],
      "metadata": {
        "id": "8sOAopH-hlM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "plot_curves(history)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5j8RlsiqhuOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the performance is clearly very poor. Additionally, our Classification Accuracy curves are indicating that the model has overfit to the training data. We have thus established our **baseline model**.\n",
        "\n",
        "Now, let's see how we can start improving the performance of the model. "
      ],
      "metadata": {
        "id": "qFaZgO2RhnqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the training dataset we are using here is very small (half of what we have in our test set), **Data Augmentation** can really help us.\n",
        "\n",
        "All modern deep learning frameworks have common data augmentation techniques built in that can make our work easier. Keras is no exception. The [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class will enable us to incorporate data augmentation into our training pipeline.\n",
        "\n",
        "We are going to use the following image transformations:\n",
        "\n",
        "*   Rotation\n",
        "*   Horizontal Flip\n",
        "*   Horizontal Translation\n",
        "*   Vertical Translation\n",
        "\n",
        "Note that the type of augmentations and amounts by which the images are transformed are *hyper-parameters* that may need to be tuned to the application and the type of data we are using by means of some search algorithm in order to achieve optimal performance:\n"
      ],
      "metadata": {
        "id": "9mIDQDncyf-R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Required import:\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load and pre-process the data:\n",
        "train_X, train_Y, test_X, test_Y = load_data()\n",
        "train_X, test_X = pre_process_data(train_X, test_X)\n",
        "\n",
        "# Create the Image Data Generator for our augmentation:\n",
        "data_generator = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    )\n",
        "\n",
        "# Create an iterator over the data:\n",
        "data_iterator = data_generator.flow(train_X, train_Y, batch_size=64)"
      ],
      "outputs": [],
      "metadata": {
        "id": "kYvHFNgHzoE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at our images post augmentation to see what they look like:"
      ],
      "metadata": {
        "id": "CeWvgHS0DjLF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# -----------------------------\n",
        "# Function that will display 16 augmented images of the dataset with their labels:\n",
        "def view_augmented_data(train_X, train_Y, class_names):\n",
        "  for i in range(16):\n",
        "    # subplot:\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # plot image:\n",
        "    plt.imshow(train_X[i])\n",
        "    class_index = train_Y[i].argmax()\n",
        "    plt.xlabel(class_names[class_index])\n",
        "\n",
        "  # show the images:\n",
        "  plt.subplots_adjust(left=0.125,\n",
        "                      bottom=0.1, \n",
        "                      right=0.9, \n",
        "                      top=0.9, \n",
        "                      wspace=0.2, \n",
        "                      hspace=0.35)\n",
        "  plt.show()\n",
        "# -----------------------------\n",
        "\n",
        "batch_X, batch_Y = next(data_iterator)\n",
        "view_augmented_data(batch_X, batch_Y, class_names)"
      ],
      "outputs": [],
      "metadata": {
        "id": "9b0ZCe5-DpBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using the [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) to perform our data augmentation.\n",
        "\n",
        "We will not change the model since our last experiment that established our baseline and will only re-initialise the model just as we did in our last experiment.\n",
        "\n",
        "This is because we want to see how data augmentation can help us improve the performance of our model and prevent overfitting.\n",
        "\n",
        "So, let's train our model **with data augmentation**:"
      ],
      "metadata": {
        "id": "b-MpkpVG1ebo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create the model:\n",
        "model = create_model()\n",
        "\n",
        "# Train the model for 50 epochs with a batch size of 64:\n",
        "history = model.fit(data_iterator, epochs=50, validation_data=(test_X, test_Y), verbose=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "5xd-lrOB1e68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training is complete. So we can now evaluate the model and take a look at our performance:"
      ],
      "metadata": {
        "id": "CzCNKAi52v09"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "_, acc = model.evaluate(test_X, test_Y, verbose=1)\n",
        "print('Accuracy: %.3f' % (acc * 100.0))\n",
        "\n",
        "plot_curves(history)"
      ],
      "outputs": [],
      "metadata": {
        "id": "lNxIp_pp24W2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how much the model has improved. The classification accuracy curves for our training and test phases are much closer to each other and the final test accuracy level is significantly higher, all thanks to data augmentation. "
      ],
      "metadata": {
        "id": "r36zrhI0T51A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "But we can do even better!!!"
      ],
      "metadata": {
        "id": "Blv1pA0VUPM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we can go with **Transfer Learning** to see how it will affect things.\n",
        "\n",
        "To keep things simple, we are going to load a VGG16 architecture with weights pre-trained on [ImageNet](http://www.image-net.org/) and then fine-tune the whole network (the final classification layer will be trained completely from scratch):"
      ],
      "metadata": {
        "id": "feKHjx9sh27I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# This function creates our classification network, this time pre-trained on ImageNet:\n",
        "def create_model():\n",
        "\n",
        "  # Keras can already provide us with a pre-trained network just by setting the weights argument to imagenet:\n",
        "  base_model = VGG16(input_shape=(32,32,3), weights='imagenet', include_top=False)\n",
        "  x = base_model.output\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  output = keras.layers.Dense(10, activation='softmax')(x)\n",
        "  model = keras.models.Model(inputs=[base_model.input], outputs=[output])\n",
        "\n",
        "  # The optimiser is stochastic gradient descent with a learning rate of 0.01 and a momentum of 0.9:\n",
        "  optim = SGD(lr=0.01, momentum=0.9)\n",
        "\n",
        "  # The model optimises cross entropy as its loss function and will monitor classification accuracy:\n",
        "  model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Printing model summary:\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "print('Done!')"
      ],
      "outputs": [],
      "metadata": {
        "id": "W6i2v_PrixdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's create and train the model. We will keep the same data augmentation from the last experiment in place, so the only change here is that the model has been **pre-trained on ImageNet**:"
      ],
      "metadata": {
        "id": "-HvlJGAqjEXk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Create the model:\n",
        "model = create_model()\n",
        "\n",
        "# Train the model for 50 epochs with a batch size of 64:\n",
        "history = model.fit(data_iterator, epochs=50, validation_data=(test_X, test_Y), verbose=1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "10WpOOl8jKtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training is complete, we can evaluate the model on the test set and plot the curves as we did before to see if we get any improvements:"
      ],
      "metadata": {
        "id": "So-yE8KIjmQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "_, acc = model.evaluate(test_X, test_Y, verbose=1)\n",
        "print('Accuracy: %.3f' % (acc * 100.0))\n",
        "\n",
        "plot_curves(history)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_HQRxYsMjynP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that the results have significantly improved. Transfer learning is an excellent idea and even in its simplest form is always recommended for most applications.\n",
        "\n",
        "ImageNet is an extremely useful dataset, and most cutting-edge state-of-the-art CNNs for different applications and tasks are often pre-trained on this dataset."
      ],
      "metadata": {
        "id": "taUtQMyrx9gK"
      }
    }
  ]
}