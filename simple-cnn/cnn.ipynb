{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiyK9v7m3uOq"
      },
      "source": [
        "# Keras code for a Convolutional Neural Network using CIFAR-10\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@newcastle.ac.uk\n",
        "\n",
        "This notebook will provide a simple example for a Convolutional Neural Network classifying CIFAR-10 data.\n",
        "\n",
        "This is a code demonstration for CSC8637: Deep Learning module, Lecture 5: Convolutional Neural Networks.\n",
        "\n",
        "Copyright (c) 2021 School of Computing, Newcastle University, UK.\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvYyOmMF776K"
      },
      "source": [
        "The **CIFAR-10** dataset contains 50,000 training images and 10,000 test images. In the following snippet, we first load and explore the dataset by visualising the first few images within the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9DHVccy2RdT"
      },
      "source": [
        "# Required imports:\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# The dataset contains 50,000 training images and 10,000 test images.\n",
        "# Loading the dataset:\n",
        "print('CIFAR-10 Dataset!')\n",
        "(train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
        "\n",
        "# CIFAR-10 contains these classes:\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'lorry']\n",
        "\n",
        "# -----------------------------\n",
        "# This function will display the first 16 images of the dataset with their labels:\n",
        "def visualize_data(train_X, train_Y, class_names):\n",
        "\n",
        "  for i in range(16):\n",
        "    # create subplot:\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    # plot image with the class name on the x-axis:\n",
        "    plt.imshow(train_X[i])\n",
        "    plt.xlabel(class_names[train_Y[i].item()])\n",
        "\n",
        "  # adjust the subplots and show the first 16 images:\n",
        "  plt.subplots_adjust(left=0.125,\n",
        "                      bottom=0.1, \n",
        "                      right=0.9, \n",
        "                      top=0.9, \n",
        "                      wspace=0.2, \n",
        "                      hspace=0.35)\n",
        "  plt.show()\n",
        "# -----------------------------\n",
        "\n",
        "# Displaying the first sixteen images within the dataset:\n",
        "visualize_data(train_X, train_Y, class_names)\n",
        "\n",
        "# Printing information about the loaded dataset:\n",
        "print(f'There are {train_X.shape[0]} images of size {train_X.shape[1:]} in the Training set of the CIFAR-10 Dataset.')\n",
        "print(f'There are {test_X.shape[0]} images of size {test_X.shape[1:]} in the Test set of the CIFAR-10 Dataset.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtyLC61r2tZh"
      },
      "source": [
        "Now that the dataset has been verified, we can begin preparing for the training process. First, let's import all that we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2qiRCTTLdpM"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print('Keras version:', keras.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khsYp2iGNWHV"
      },
      "source": [
        "We now define two functions that will load and process the data in preparation for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwcJww6zNwOx"
      },
      "source": [
        "# Function used to load the dataset:\n",
        "def load_data():\n",
        "\n",
        "  # Loading the built-in CIFAR-10 dataset from Keras:\n",
        "  (train_X, train_Y), (test_X, test_Y) = cifar10.load_data()\n",
        "\n",
        "  print('The dataset has been loaded:')\n",
        "  print(f'Train: X={train_X.shape}, Y={train_Y.shape}')\n",
        "  print(f'Test: X={test_X.shape}, Y={test_Y.shape}')\n",
        "\n",
        "  # Converting the class labels to one hot vectors:\n",
        "  train_Y = to_categorical(train_Y)\n",
        "  test_Y = to_categorical(test_Y)\n",
        "\n",
        "  return train_X, train_Y, test_X, test_Y\n",
        " \n",
        "# -----------------------------\n",
        "\n",
        "# Function used to prepare the data:\n",
        "def pre_process_data(train_data, test_data):\n",
        "\n",
        "  # Casting pixel values to floats:\n",
        "  train_data = train_data.astype('float32')\n",
        "  test_data = test_data.astype('float32')\n",
        "\n",
        "  # normalising pixel values to range [0-1]\n",
        "  train_data = train_data / 255.0\n",
        "  test_data = test_data / 255.0\n",
        "\n",
        "  print(f'Train data is in range {train_data.min()} to {train_data.max()}.')\n",
        "  print(f'Test data is in range {test_data.min()} to {test_data.max()}.')\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDBCjcmWbxzv"
      },
      "source": [
        "We will also define a function responsible for plotting the curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rnUNcFYb5iM"
      },
      "source": [
        "# Function used to plot the curves for loss and accuracy:\n",
        "def plot_curves(history):\n",
        "\n",
        "  # Plotting the loss curve:\n",
        "  plt.subplot(211)\n",
        "  plt.title('Cross Entropy')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  # Plotting the training loss (blue):\n",
        "  plt.plot(history.history['loss'], color='blue', label='train')\n",
        "  # Plotting the test loss (red):\n",
        "  plt.plot(history.history['val_loss'], color='red', label='test')\n",
        "  # Legend for the plot:\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  # Plotting the accuracy curve:\n",
        "  plt.subplot(212)\n",
        "  plt.title('Classification Accuracy')\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  # Plotting the training accuracy (blue):\n",
        "  plt.plot(history.history['accuracy'], color='blue', label='train')\n",
        "  # Plotting the test accuracy (red):\n",
        "  plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
        "  # Legend for the plot:\n",
        "  plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "  plt.subplots_adjust(top=3)\n",
        "  plt.show()\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6TnX_H7Sgt4"
      },
      "source": [
        "Now we are ready to design our model architecture.\n",
        "\n",
        "The architecture comprises **three CONV layers** with **RELU activation functions**, each followed by **Max Pooling** layers. At the end, there is a **fully-connected** classifier that will classify the input into one of 10 outputs, using **cross entropy** as the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA3x4-5JSmaw"
      },
      "source": [
        "# This function defines our neural network:\n",
        "def create_model():\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # The first conv layer with 32 kernels of 3*3 receiving an input of 32*32*3:\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "\n",
        "  # Max pooling layer with a kernel of 2*2 and a stride of 2:\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  # Conv layer with 64 kernels of 3*3:\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\n",
        "  # Max pooling layer with a kernel of 2*2 and a stride of 2:\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  # Conv layer with 128 kernels of 3*3:\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\n",
        "  # Max pooling layer with a kernel of 2*2 and a stride of 2:\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  # The feature maps are flattened at this point to be passed into fully-connected layers:\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # Fully-connected layers leading to 10 classes with a softmax activation function:\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # The optimiser is stochastic gradient descent with a learning rate of f 0.001 and a momentum of 0.9:\n",
        "  optim = SGD(lr=0.001, momentum=0.9)\n",
        "\n",
        "  # The model optimises cross entropy as its loss function and will monitor classification accuracy:\n",
        "  model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Printing model summary:\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7_ukGSQc4mz"
      },
      "source": [
        "Now that all preparations are made and the model has been designed, it is time to start training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn3VXARvdE7z"
      },
      "source": [
        "First, let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxLL8PGddJfW"
      },
      "source": [
        "trainX, trainY, testX, testY = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5HA9zq5dhcq"
      },
      "source": [
        "Now, we pre-process the images using the function we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl05vwsEduA9"
      },
      "source": [
        "trainX, testX = pre_process_data(trainX, testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xztA7dmCfkLB"
      },
      "source": [
        "Let's create the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2bH8teBfmq9"
      },
      "source": [
        "model = create_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lIJ2EJueYZC"
      },
      "source": [
        "The model can now be trained for 20 epochs with a batch size of 64:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNosfjAueY4q"
      },
      "source": [
        "history = model.fit(trainX, trainY, epochs=20, batch_size=64, validation_data=(testX, testY))\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44EK0qiahETH"
      },
      "source": [
        "After the training is complete, we can evaluate the model on the test set and obtain the final accuracy level:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIDsBvm2hLbN"
      },
      "source": [
        "_, acc = model.evaluate(testX, testY, verbose=1)\n",
        "print('Accuracy: %.3f' % (acc * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sOAopH-hlM7"
      },
      "source": [
        "We can plot the loss and accuracy curves to better analyse the training process.\n",
        "\n",
        "The **blue** curves indicate performance over the **training data** and the *red* curves represent model performance over the *test data*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j8RlsiqhuOo"
      },
      "source": [
        "plot_curves(history)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}