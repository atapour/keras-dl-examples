{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture13_Features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiyK9v7m3uOq"
      },
      "source": [
        "# Keras Code for Visualising Activation Maps\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@newcastle.ac.uk\n",
        "\n",
        "This notebook will provide an example for visualising activation maps from different blocks of a VGG-16.\n",
        "\n",
        "This is a code demonstration for CSC8637: Deep Learning module, Lecture 13: Ethics and Challenges.\n",
        "\n",
        "Copyright (c) 2021 School of Computing, Newcastle University, UK.\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvYyOmMF776K"
      },
      "source": [
        "In this demo, we are going to visualise the activation maps from different layers of a VGG-16 network trained on ImageNet. The same process can of course be done for architectures other than VGG-16."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TGuACtaAtNH"
      },
      "source": [
        "Let's start by importing what we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9DHVccy2RdT"
      },
      "source": [
        "# Required imports:\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "from matplotlib import pyplot\n",
        "from numpy import expand_dims\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "print('Keras version:', keras.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtyLC61r2tZh"
      },
      "source": [
        "First, we need an image to pass through our network. To make the process easier, we will get an image from the internet, but you can do this with any image your heart desires. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2qiRCTTLdpM"
      },
      "source": [
        "# URL to download the image. Make sure this is a link to actual image (with the extension at the end):\n",
        "url = 'https://images.freeimages.com/images/large-previews/006/young-dachshund-1362378.jpg'\n",
        "\n",
        "# Read the image using skimage:\n",
        "image = io.imread(url)\n",
        "\n",
        "# Resize the image to 224x224, since this is what most CNNs expect. You might need to change the size depending on the network you use:\n",
        "image = resize(image, (224, 224), anti_aliasing=True)\n",
        "\n",
        "# Display the image:\n",
        "pyplot.axis('off')\n",
        "pyplot.imshow(image) \n",
        "print(f'Image with size {image.shape} has been loaded.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khsYp2iGNWHV"
      },
      "source": [
        "We now need to process the image so it can accepted by our neural network. Here, we will use the function built into Keras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwcJww6zNwOx"
      },
      "source": [
        "# Convert the image to an array:\n",
        "image = img_to_array(image)\n",
        "# The model expects a 4D tensor so expand dimensions:\n",
        "image = expand_dims(image, axis=0)\n",
        "# Scale pixel values:\n",
        "image = preprocess_input(image)\n",
        "\n",
        "print(f'Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDq9fa_ghiTs"
      },
      "source": [
        "We need to define a function that displays the activation maps we extract from the layer we want in a square grid:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLmLSuBzR4nL"
      },
      "source": [
        "# Function to display the activation maps:\n",
        "def display(feature_maps, index, square):\n",
        "  # Get the feature maps for the layer we are interested in:\n",
        "  fmap = feature_maps[index]\n",
        "  # Plot 2^square feature maps\n",
        "  ix = 1\n",
        "\n",
        "  for _ in range(square):\n",
        "    for _ in range(square):\n",
        "      # Create subplots:\n",
        "      ax = pyplot.subplot(square, square, ix)\n",
        "      # Turn off axis:\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "      # Extract and enhance filter channel using PIL\n",
        "      img = fmap[0, :, :, ix-1]\n",
        "      # Convert image to PIL:\n",
        "      im = Image.fromarray(img)\n",
        "      # Convert image to 3-channel for the enhancer:\n",
        "      im = im.convert('RGB')\n",
        "      # Create enhancer:\n",
        "      enhancer = ImageEnhance.Contrast(im)\n",
        "      # Increase the contrast by:\n",
        "      factor = 10\n",
        "      # Enhance (increase contrast):\n",
        "      im_output = enhancer.enhance(factor)\n",
        "      # Convert the image back to array:\n",
        "      img = np.array(im_output)\n",
        "      # Plot the images:\n",
        "      pyplot.imshow(img[:,:,0], cmap='plasma')\n",
        "      ix += 1\n",
        "  # Show figure\n",
        "  pyplot.show()\n",
        "\n",
        "print(f'Done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDBCjcmWbxzv"
      },
      "source": [
        "Now, let's choose our model architecture. We are going to use VGG-16 here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rnUNcFYb5iM"
      },
      "source": [
        "model = VGG16()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6TnX_H7Sgt4"
      },
      "source": [
        "We are going to visulaise feature maps from each convolutional block within VGG16. We will do this by passing the image through the image once and outputting the feature maps from the layers we want.\n",
        "\n",
        "The convolutional layers at the end of each of the 5 blocks are where we extract our feature maps. The indices of these layers are [2, 5, 9, 13, 17]. Note that this only applies to VGG16.\n",
        "\n",
        "We will have to re-define the network to output the features we are looking for:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBal6PxRRpv5"
      },
      "source": [
        "# Re-define the network to output feature maps at the chosen layers:\n",
        "layers = [2, 5, 9, 13, 17]\n",
        "outputs = [model.layers[i].output for i in layers]\n",
        "model = Model(inputs=model.inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wG4N3HrSf0c"
      },
      "source": [
        "Now, let's pass the image through the network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA3x4-5JSmaw"
      },
      "source": [
        "feature_maps = model.predict(image)\n",
        "print('done!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7_ukGSQc4mz"
      },
      "source": [
        "It is now time to display what we got. First, let's look at the output of the first 16 filters of the first block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvrf7i0sfMte"
      },
      "source": [
        "display(feature_maps=feature_maps, index=0, square=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VqWxGpmiNgE"
      },
      "source": [
        "Now, the second convolutional block:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVJQl2FjiOFe"
      },
      "source": [
        "display(feature_maps=feature_maps, index=1, square=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZT5nLmAidkr"
      },
      "source": [
        "The third:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSbq8aY_ifpa"
      },
      "source": [
        "display(feature_maps=feature_maps, index=2, square=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tKOqT6PigZh"
      },
      "source": [
        "Fourth:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJ-0DKnijSx"
      },
      "source": [
        "display(feature_maps=feature_maps, index=3, square=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPx1ZNz9ij9s"
      },
      "source": [
        "And now the last one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXHpqqGrioeJ"
      },
      "source": [
        "display(feature_maps=feature_maps, index=4, square=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viOYEp0Qi1QD"
      },
      "source": [
        "As you see, the deeper into the network we go, the harder it gets to actually see the maps."
      ]
    }
  ]
}